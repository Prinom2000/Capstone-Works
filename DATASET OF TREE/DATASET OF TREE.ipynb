{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SPARQLWrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lT94roBmjEl",
        "outputId": "bb8ff498-4b35-4f52-d7f8-8c8f355dd0f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
            "  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting isodate<1.0.0,>=0.7.2 (from rdflib>=6.1.1->SPARQLWrapper)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.0)\n",
            "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading rdflib-7.1.1-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, rdflib, SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-2.0.0 isodate-0.7.2 rdflib-7.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9TkDvobymSua"
      },
      "outputs": [],
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Define query parameters\n",
        "rows_per_query = 50  # Number of rows to fetch per query\n",
        "total_records = 100  # Total number of records to fetch\n",
        "retry_delay = 5  # Delay in seconds before retrying on failure\n",
        "\n",
        "# SPARQL query loop\n",
        "for offset in range(0, total_records, rows_per_query):\n",
        "    print(f\"Fetching rows {offset + 1} to {offset + rows_per_query}...\")\n",
        "\n",
        "    # SPARQL query with pagination\n",
        "    sparql.setQuery(f\"\"\"\n",
        "        SELECT ?tree ?name ?family ?genus ?order ?nativeArea ?conservationStatus\n",
        "        WHERE {{\n",
        "            ?tree a dbo:Plant .\n",
        "            ?tree foaf:name ?name .\n",
        "            OPTIONAL {{ ?tree dbo:family ?family . }}\n",
        "            OPTIONAL {{ ?tree dbo:genus ?genus . }}\n",
        "            OPTIONAL {{ ?tree dbo:order ?order . }}\n",
        "            OPTIONAL {{ ?tree dbo:nativeArea ?nativeArea . }}\n",
        "            OPTIONAL {{ ?tree dbo:conservationStatus ?conservationStatus . }}\n",
        "            FILTER (lang(?name) = 'en')  # Ensure English results\n",
        "        }}\n",
        "        LIMIT {rows_per_query} OFFSET {offset}\n",
        "    \"\"\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    success = False\n",
        "    retries = 3  # Retry count\n",
        "\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            # Execute query\n",
        "            results = sparql.query().convert()\n",
        "            for result in results[\"results\"][\"bindings\"]:\n",
        "                # Append result to data\n",
        "                data.append({\n",
        "                    \"Tree\": result[\"tree\"][\"value\"],\n",
        "                    \"Name\": result[\"name\"][\"value\"],\n",
        "                    \"Family\": result.get(\"family\", {}).get(\"value\"),\n",
        "                    \"Genus\": result.get(\"genus\", {}).get(\"value\"),\n",
        "                    \"Order\": result.get(\"order\", {}).get(\"value\"),\n",
        "                    \"Native Region\": result.get(\"nativeArea\", {}).get(\"value\"),\n",
        "                    \"Conservation Status\": result.get(\"conservationStatus\", {}).get(\"value\"),\n",
        "                })\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred for offset {offset}: {e}\")\n",
        "            retries -= 1\n",
        "            if retries > 0:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(\"Skipping this batch after multiple failed attempts.\")\n",
        "\n",
        "    # Add a delay to avoid rate-limiting\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert collected data to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv(\"tree_dataset.csv\", index=False)\n",
        "print(\"Dataset saved as tree_dataset.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75jWVB0YmXZH",
        "outputId": "c38f0b26-d36e-4e88-ebfd-80b3f9bb1d42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching rows 1 to 50...\n",
            "Fetching rows 51 to 100...\n",
            "Dataset saved as tree_dataset.csv\n",
            "                                                Tree                    Name  \\\n",
            "0       http://dbpedia.org/resource/American_ginseng        American ginseng   \n",
            "1       http://dbpedia.org/resource/American_ginseng        American ginseng   \n",
            "2       http://dbpedia.org/resource/American_ginseng        American ginseng   \n",
            "3       http://dbpedia.org/resource/American_ginseng        American ginseng   \n",
            "4  http://dbpedia.org/resource/Amesiella_philippe...  Amesiella philippensis   \n",
            "\n",
            "                                       Family  \\\n",
            "0     http://dbpedia.org/resource/Aralioideae   \n",
            "1      http://dbpedia.org/resource/Araliaceae   \n",
            "2     http://dbpedia.org/resource/Aralioideae   \n",
            "3      http://dbpedia.org/resource/Araliaceae   \n",
            "4  http://dbpedia.org/resource/Epidendroideae   \n",
            "\n",
            "                                   Genus  \\\n",
            "0      http://dbpedia.org/resource/Panax   \n",
            "1      http://dbpedia.org/resource/Panax   \n",
            "2      http://dbpedia.org/resource/Panax   \n",
            "3      http://dbpedia.org/resource/Panax   \n",
            "4  http://dbpedia.org/resource/Amesiella   \n",
            "\n",
            "                                     Order Native Region Conservation Status  \n",
            "0     http://dbpedia.org/resource/Asterids          None                  G3  \n",
            "1     http://dbpedia.org/resource/Asterids          None                  G3  \n",
            "2      http://dbpedia.org/resource/Apiales          None                  G3  \n",
            "3      http://dbpedia.org/resource/Apiales          None                  G3  \n",
            "4  http://dbpedia.org/resource/Asparagales          None                  EN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Define query parameters\n",
        "rows_per_query = 50  # Number of rows to fetch per query\n",
        "total_records = 100  # Total number of records to fetch\n",
        "retry_delay = 5  # Delay in seconds before retrying on failure\n",
        "\n",
        "# SPARQL query loop\n",
        "for offset in range(0, total_records, rows_per_query):\n",
        "    print(f\"Fetching rows {offset + 1} to {offset + rows_per_query}...\")\n",
        "\n",
        "    # SPARQL query with pagination (fetch only ?tree)\n",
        "    sparql.setQuery(f\"\"\"\n",
        "        SELECT DISTINCT ?tree\n",
        "        WHERE {{\n",
        "            ?tree a dbo:Plant .\n",
        "        }}\n",
        "        LIMIT {rows_per_query} OFFSET {offset}\n",
        "    \"\"\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    success = False\n",
        "    retries = 3  # Retry count\n",
        "\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            # Execute query\n",
        "            results = sparql.query().convert()\n",
        "            for result in results[\"results\"][\"bindings\"]:\n",
        "                # Append only the Tree link\n",
        "                data.append({\"Tree\": result[\"tree\"][\"value\"]})\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred for offset {offset}: {e}\")\n",
        "            retries -= 1\n",
        "            if retries > 0:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(\"Skipping this batch after multiple failed attempts.\")\n",
        "\n",
        "    # Add a delay to avoid rate-limiting\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert collected data to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv(\"tree_dataset_100.csv\", index=False)\n",
        "print(\"tree_dataset_100.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rLdj-7dnl6u",
        "outputId": "44f8b910-3c62-480e-c3c3-a56965a46ea5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching rows 1 to 50...\n",
            "Fetching rows 51 to 100...\n",
            "tree_dataset_100.csv\n",
            "                                               Tree\n",
            "0      http://dbpedia.org/resource/Cadaba_insularis\n",
            "1  http://dbpedia.org/resource/Caesalpinia_coriaria\n",
            "2          http://dbpedia.org/resource/Caia_(plant)\n",
            "3              http://dbpedia.org/resource/Calabash\n",
            "4     http://dbpedia.org/resource/Caladenia_lyallii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Define query parameters\n",
        "rows_per_query = 100  # Number of rows to fetch per query\n",
        "total_records = 1000  # Total number of records to fetch\n",
        "retry_delay = 5  # Delay in seconds before retrying on failure\n",
        "\n",
        "# SPARQL query loop\n",
        "for offset in range(0, total_records, rows_per_query):\n",
        "    print(f\"Fetching rows {offset + 1} to {offset + rows_per_query}...\")\n",
        "\n",
        "    # SPARQL query with pagination (fetch only ?tree)\n",
        "    sparql.setQuery(f\"\"\"\n",
        "        SELECT DISTINCT ?tree\n",
        "        WHERE {{\n",
        "            ?tree a dbo:Plant .\n",
        "        }}\n",
        "        LIMIT {rows_per_query} OFFSET {offset}\n",
        "    \"\"\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    success = False\n",
        "    retries = 3  # Retry count\n",
        "\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            # Execute query\n",
        "            results = sparql.query().convert()\n",
        "            for result in results[\"results\"][\"bindings\"]:\n",
        "                # Append only the Tree link\n",
        "                data.append({\"Tree\": result[\"tree\"][\"value\"]})\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred for offset {offset}: {e}\")\n",
        "            retries -= 1\n",
        "            if retries > 0:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(\"Skipping this batch after multiple failed attempts.\")\n",
        "\n",
        "    # Add a delay to avoid rate-limiting\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert collected data to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv(\"tree_dataset_1000.csv\", index=False)\n",
        "print(\"tree_dataset_1000.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyHiyrv8nwEf",
        "outputId": "ade9b92d-231e-4770-e15b-33f78f4be739"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching rows 1 to 100...\n",
            "Fetching rows 101 to 200...\n",
            "Fetching rows 201 to 300...\n",
            "Fetching rows 301 to 400...\n",
            "Fetching rows 401 to 500...\n",
            "Fetching rows 501 to 600...\n",
            "Fetching rows 601 to 700...\n",
            "Fetching rows 701 to 800...\n",
            "Fetching rows 801 to 900...\n",
            "Fetching rows 901 to 1000...\n",
            "tree_dataset_1000.csv\n",
            "                                               Tree\n",
            "0      http://dbpedia.org/resource/Cadaba_insularis\n",
            "1  http://dbpedia.org/resource/Caesalpinia_coriaria\n",
            "2          http://dbpedia.org/resource/Caia_(plant)\n",
            "3              http://dbpedia.org/resource/Calabash\n",
            "4     http://dbpedia.org/resource/Caladenia_lyallii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Define query parameters\n",
        "rows_per_query = 500  # Number of rows to fetch per query\n",
        "total_records = 10000  # Total number of records to fetch\n",
        "retry_delay = 5  # Delay in seconds before retrying on failure\n",
        "\n",
        "# SPARQL query loop\n",
        "for offset in range(0, total_records, rows_per_query):\n",
        "    print(f\"Fetching rows {offset + 1} to {offset + rows_per_query}...\")\n",
        "\n",
        "    # SPARQL query with pagination (fetch only ?tree)\n",
        "    sparql.setQuery(f\"\"\"\n",
        "        SELECT DISTINCT ?tree\n",
        "        WHERE {{\n",
        "            ?tree a dbo:Plant .\n",
        "        }}\n",
        "        LIMIT {rows_per_query} OFFSET {offset}\n",
        "    \"\"\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    success = False\n",
        "    retries = 3  # Retry count\n",
        "\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            # Execute query\n",
        "            results = sparql.query().convert()\n",
        "            for result in results[\"results\"][\"bindings\"]:\n",
        "                # Append only the Tree link\n",
        "                data.append({\"Tree\": result[\"tree\"][\"value\"]})\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred for offset {offset}: {e}\")\n",
        "            retries -= 1\n",
        "            if retries > 0:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(\"Skipping this batch after multiple failed attempts.\")\n",
        "\n",
        "    # Add a delay to avoid rate-limiting\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert collected data to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv(\"tree_dataset_10K.csv\", index=False)\n",
        "print(\"tree_dataset_10K.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdFuQVCwoT0D",
        "outputId": "9796bcda-ddaa-4d8a-d359-8b1fe70964c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching rows 1 to 500...\n",
            "Fetching rows 501 to 1000...\n",
            "Fetching rows 1001 to 1500...\n",
            "Fetching rows 1501 to 2000...\n",
            "Fetching rows 2001 to 2500...\n",
            "Fetching rows 2501 to 3000...\n",
            "Fetching rows 3001 to 3500...\n",
            "Fetching rows 3501 to 4000...\n",
            "Fetching rows 4001 to 4500...\n",
            "Fetching rows 4501 to 5000...\n",
            "Fetching rows 5001 to 5500...\n",
            "Fetching rows 5501 to 6000...\n",
            "Fetching rows 6001 to 6500...\n",
            "Fetching rows 6501 to 7000...\n",
            "Fetching rows 7001 to 7500...\n",
            "Fetching rows 7501 to 8000...\n",
            "Fetching rows 8001 to 8500...\n",
            "Fetching rows 8501 to 9000...\n",
            "Fetching rows 9001 to 9500...\n",
            "Fetching rows 9501 to 10000...\n",
            "tree_dataset_10K.csv\n",
            "                                               Tree\n",
            "0      http://dbpedia.org/resource/Cadaba_insularis\n",
            "1  http://dbpedia.org/resource/Caesalpinia_coriaria\n",
            "2          http://dbpedia.org/resource/Caia_(plant)\n",
            "3              http://dbpedia.org/resource/Calabash\n",
            "4     http://dbpedia.org/resource/Caladenia_lyallii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Set up SPARQL endpoint\n",
        "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
        "\n",
        "# Initialize an empty list to store data\n",
        "data = []\n",
        "\n",
        "# Define query parameters\n",
        "rows_per_query = 500  # Number of rows to fetch per query\n",
        "total_records = 1000000  # Total number of records to fetch\n",
        "retry_delay = 5  # Delay in seconds before retrying on failure\n",
        "\n",
        "# SPARQL query loop\n",
        "for offset in range(0, total_records, rows_per_query):\n",
        "    print(f\"Fetching rows {offset + 1} to {offset + rows_per_query}...\")\n",
        "\n",
        "    # SPARQL query with pagination (fetch only ?tree)\n",
        "    sparql.setQuery(f\"\"\"\n",
        "        SELECT DISTINCT ?tree\n",
        "        WHERE {{\n",
        "            ?tree a dbo:Plant .\n",
        "        }}\n",
        "        LIMIT {rows_per_query} OFFSET {offset}\n",
        "    \"\"\")\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    success = False\n",
        "    retries = 3  # Retry count\n",
        "\n",
        "    while not success and retries > 0:\n",
        "        try:\n",
        "            # Execute query\n",
        "            results = sparql.query().convert()\n",
        "            for result in results[\"results\"][\"bindings\"]:\n",
        "                # Append only the Tree link\n",
        "                data.append({\"Tree\": result[\"tree\"][\"value\"]})\n",
        "            success = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error occurred for offset {offset}: {e}\")\n",
        "            retries -= 1\n",
        "            if retries > 0:\n",
        "                print(f\"Retrying in {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "            else:\n",
        "                print(\"Skipping this batch after multiple failed attempts.\")\n",
        "\n",
        "    # Add a delay to avoid rate-limiting\n",
        "    time.sleep(1)\n",
        "\n",
        "# Convert collected data to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the dataset to a CSV file\n",
        "df.to_csv(\"tree_dataset_1M.csv\", index=False)\n",
        "print(\"tree_dataset_1M.csv\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmxhk3SNofAQ",
        "outputId": "707b8cfd-b727-46d7-a4d5-cb65316a176a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching rows 1 to 500...\n",
            "Fetching rows 501 to 1000...\n",
            "Fetching rows 1001 to 1500...\n",
            "Fetching rows 1501 to 2000...\n",
            "Fetching rows 2001 to 2500...\n",
            "Fetching rows 2501 to 3000...\n",
            "Fetching rows 3001 to 3500...\n",
            "Fetching rows 3501 to 4000...\n",
            "Fetching rows 4001 to 4500...\n",
            "Fetching rows 4501 to 5000...\n",
            "Fetching rows 5001 to 5500...\n",
            "Fetching rows 5501 to 6000...\n",
            "Fetching rows 6001 to 6500...\n",
            "Fetching rows 6501 to 7000...\n",
            "Fetching rows 7001 to 7500...\n",
            "Fetching rows 7501 to 8000...\n",
            "Fetching rows 8001 to 8500...\n",
            "Fetching rows 8501 to 9000...\n",
            "Fetching rows 9001 to 9500...\n",
            "Fetching rows 9501 to 10000...\n",
            "Fetching rows 10001 to 10500...\n",
            "Fetching rows 10501 to 11000...\n",
            "Fetching rows 11001 to 11500...\n",
            "Fetching rows 11501 to 12000...\n",
            "Fetching rows 12001 to 12500...\n",
            "Fetching rows 12501 to 13000...\n",
            "Fetching rows 13001 to 13500...\n",
            "Fetching rows 13501 to 14000...\n",
            "Fetching rows 14001 to 14500...\n",
            "Fetching rows 14501 to 15000...\n",
            "Fetching rows 15001 to 15500...\n",
            "Fetching rows 15501 to 16000...\n",
            "Fetching rows 16001 to 16500...\n",
            "Fetching rows 16501 to 17000...\n",
            "Fetching rows 17001 to 17500...\n",
            "Fetching rows 17501 to 18000...\n",
            "Fetching rows 18001 to 18500...\n",
            "Fetching rows 18501 to 19000...\n",
            "Fetching rows 19001 to 19500...\n",
            "Fetching rows 19501 to 20000...\n",
            "Fetching rows 20001 to 20500...\n",
            "Fetching rows 20501 to 21000...\n",
            "Fetching rows 21001 to 21500...\n",
            "Fetching rows 21501 to 22000...\n",
            "Fetching rows 22001 to 22500...\n",
            "Fetching rows 22501 to 23000...\n",
            "Fetching rows 23001 to 23500...\n",
            "Fetching rows 23501 to 24000...\n",
            "Fetching rows 24001 to 24500...\n",
            "Fetching rows 24501 to 25000...\n",
            "Fetching rows 25001 to 25500...\n",
            "Fetching rows 25501 to 26000...\n",
            "Fetching rows 26001 to 26500...\n",
            "Fetching rows 26501 to 27000...\n",
            "Fetching rows 27001 to 27500...\n",
            "Fetching rows 27501 to 28000...\n",
            "Fetching rows 28001 to 28500...\n",
            "Fetching rows 28501 to 29000...\n",
            "Fetching rows 29001 to 29500...\n",
            "Fetching rows 29501 to 30000...\n",
            "Fetching rows 30001 to 30500...\n",
            "Fetching rows 30501 to 31000...\n",
            "Fetching rows 31001 to 31500...\n",
            "Fetching rows 31501 to 32000...\n",
            "Fetching rows 32001 to 32500...\n",
            "Fetching rows 32501 to 33000...\n",
            "Fetching rows 33001 to 33500...\n",
            "Fetching rows 33501 to 34000...\n",
            "Fetching rows 34001 to 34500...\n",
            "Fetching rows 34501 to 35000...\n",
            "Fetching rows 35001 to 35500...\n",
            "Fetching rows 35501 to 36000...\n",
            "Fetching rows 36001 to 36500...\n",
            "Fetching rows 36501 to 37000...\n",
            "Fetching rows 37001 to 37500...\n",
            "Fetching rows 37501 to 38000...\n",
            "Fetching rows 38001 to 38500...\n",
            "Fetching rows 38501 to 39000...\n",
            "Fetching rows 39001 to 39500...\n",
            "Fetching rows 39501 to 40000...\n",
            "Fetching rows 40001 to 40500...\n",
            "Fetching rows 40501 to 41000...\n",
            "Fetching rows 41001 to 41500...\n",
            "Fetching rows 41501 to 42000...\n",
            "Fetching rows 42001 to 42500...\n",
            "Fetching rows 42501 to 43000...\n",
            "Fetching rows 43001 to 43500...\n",
            "Fetching rows 43501 to 44000...\n",
            "Fetching rows 44001 to 44500...\n",
            "Fetching rows 44501 to 45000...\n",
            "Fetching rows 45001 to 45500...\n",
            "Fetching rows 45501 to 46000...\n",
            "Fetching rows 46001 to 46500...\n",
            "Fetching rows 46501 to 47000...\n",
            "Fetching rows 47001 to 47500...\n",
            "Fetching rows 47501 to 48000...\n",
            "Fetching rows 48001 to 48500...\n",
            "Fetching rows 48501 to 49000...\n",
            "Fetching rows 49001 to 49500...\n",
            "Fetching rows 49501 to 50000...\n",
            "Fetching rows 50001 to 50500...\n",
            "Fetching rows 50501 to 51000...\n",
            "Fetching rows 51001 to 51500...\n",
            "Fetching rows 51501 to 52000...\n",
            "Fetching rows 52001 to 52500...\n",
            "Fetching rows 52501 to 53000...\n",
            "Fetching rows 53001 to 53500...\n",
            "Fetching rows 53501 to 54000...\n",
            "Fetching rows 54001 to 54500...\n",
            "Fetching rows 54501 to 55000...\n",
            "Fetching rows 55001 to 55500...\n",
            "Fetching rows 55501 to 56000...\n",
            "Fetching rows 56001 to 56500...\n",
            "Fetching rows 56501 to 57000...\n",
            "Fetching rows 57001 to 57500...\n",
            "Fetching rows 57501 to 58000...\n",
            "Fetching rows 58001 to 58500...\n",
            "Fetching rows 58501 to 59000...\n",
            "Fetching rows 59001 to 59500...\n",
            "Fetching rows 59501 to 60000...\n",
            "Fetching rows 60001 to 60500...\n",
            "Fetching rows 60501 to 61000...\n",
            "Fetching rows 61001 to 61500...\n",
            "Fetching rows 61501 to 62000...\n",
            "Fetching rows 62001 to 62500...\n",
            "Fetching rows 62501 to 63000...\n",
            "Fetching rows 63001 to 63500...\n",
            "Fetching rows 63501 to 64000...\n",
            "Fetching rows 64001 to 64500...\n",
            "Fetching rows 64501 to 65000...\n",
            "Fetching rows 65001 to 65500...\n",
            "Fetching rows 65501 to 66000...\n",
            "Fetching rows 66001 to 66500...\n",
            "Fetching rows 66501 to 67000...\n",
            "Fetching rows 67001 to 67500...\n",
            "Fetching rows 67501 to 68000...\n",
            "Fetching rows 68001 to 68500...\n",
            "Fetching rows 68501 to 69000...\n",
            "Fetching rows 69001 to 69500...\n",
            "Fetching rows 69501 to 70000...\n",
            "Fetching rows 70001 to 70500...\n",
            "Fetching rows 70501 to 71000...\n",
            "Fetching rows 71001 to 71500...\n",
            "Fetching rows 71501 to 72000...\n",
            "Fetching rows 72001 to 72500...\n",
            "Fetching rows 72501 to 73000...\n",
            "Fetching rows 73001 to 73500...\n",
            "Fetching rows 73501 to 74000...\n",
            "Fetching rows 74001 to 74500...\n",
            "Fetching rows 74501 to 75000...\n",
            "Fetching rows 75001 to 75500...\n",
            "Fetching rows 75501 to 76000...\n",
            "Fetching rows 76001 to 76500...\n",
            "Fetching rows 76501 to 77000...\n",
            "Fetching rows 77001 to 77500...\n",
            "Fetching rows 77501 to 78000...\n",
            "Fetching rows 78001 to 78500...\n",
            "Fetching rows 78501 to 79000...\n",
            "Fetching rows 79001 to 79500...\n",
            "Fetching rows 79501 to 80000...\n",
            "Fetching rows 80001 to 80500...\n",
            "Fetching rows 80501 to 81000...\n",
            "Fetching rows 81001 to 81500...\n",
            "Fetching rows 81501 to 82000...\n",
            "Fetching rows 82001 to 82500...\n",
            "Fetching rows 82501 to 83000...\n",
            "Fetching rows 83001 to 83500...\n",
            "Fetching rows 83501 to 84000...\n",
            "Fetching rows 84001 to 84500...\n",
            "Fetching rows 84501 to 85000...\n",
            "Fetching rows 85001 to 85500...\n",
            "Fetching rows 85501 to 86000...\n",
            "Fetching rows 86001 to 86500...\n",
            "Fetching rows 86501 to 87000...\n",
            "Fetching rows 87001 to 87500...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UJFk9f8gosMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}